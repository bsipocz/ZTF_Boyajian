{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import axs\n",
    "import numpy as np\n",
    "\n",
    "from astropy.io import fits\n",
    "import astropy.coordinates as coord\n",
    "import astropy.units as u\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyspark.sql.functions as sparkfunc\n",
    "import pyspark.sql.types as pyspark_types\n",
    "import numpy as np\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_start(local_dir):\n",
    "    from pyspark.sql import SparkSession\n",
    "    \n",
    "    spark = (\n",
    "            SparkSession.builder\n",
    "            .appName(\"LSD2\")\n",
    "            .config(\"spark.sql.warehouse.dir\", local_dir)\n",
    "            .config('spark.master', \"local[6]\")\n",
    "            .config('spark.driver.memory', '8G') # 128\n",
    "            .config('spark.local.dir', local_dir)\n",
    "            .config('spark.memory.offHeap.enabled', 'true')\n",
    "            .config('spark.memory.offHeap.size', '4G') # 256\n",
    "            .config(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "            .config(\"spark.driver.maxResultSize\", \"6G\")\n",
    "            .config(\"spark.driver.extraJavaOptions\", f\"-Dderby.system.home={local_dir}\")\n",
    "            .enableHiveSupport()\n",
    "            .getOrCreate()\n",
    "                    )   \n",
    "\n",
    "    return spark\n",
    "\n",
    "spark_session = spark_start(\"/epyc/users/kyboone/spark-tmp/\")\n",
    "\n",
    "catalog = axs.AxsCatalog(spark_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://epyc.astro.washington.edu:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0-SNAPSHOT</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[6]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>LSD2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f89c17f5c88>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hovering over \"Spark UI\" above gives you the port number of the Spark web dashboard.  Epyc doesn't have that port open, though, so we use an SSH tunnel to forward the ports.  I like to put the following function into my `.bashrc` o my local machine:\n",
    "\n",
    "\n",
    "```\n",
    "function spark_tunnel()\n",
    "{\n",
    "        # this function takes one argument: the epyc port to tunnel\n",
    "        # the ordering is backwards (requiring a manual refresh) because\n",
    "        # I want to be able to manually kill the ssh tunnel\n",
    "        open http://localhost:${1}/\n",
    "        ssh -N -L ${1}:127.0.0.1:${1} username@epyc.astro.washington.edu\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What tables does AXS know about?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load ZTF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ztf = catalog.load('ztf_mar19_all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lightcurve(row):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    for filterid in np.unique(row['filterid']):\n",
    "        cut = (\n",
    "            (np.array(row['filterid']) == filterid)\n",
    "            & (np.array(row['catflags']) == 0.)\n",
    "        )\n",
    "        def cc(x):\n",
    "            return np.array(x)[cut]\n",
    "        plt.errorbar(cc(np.array(row['mjd'])), cc(row['psfmag']), cc(row['psfmagerr']), fmt='o', c='C%d' % filterid, label='Filter %d' % filterid)\n",
    "\n",
    "    plt.xlabel('mjd')\n",
    "    plt.ylabel('Magnitude')\n",
    "    plt.legend()\n",
    "    plt.title('matchid %d' % row['matchid'])\n",
    "    plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cython setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cython_function():\n",
    "    def __init__(self, module, name):\n",
    "        self.module = module\n",
    "        self.name = name\n",
    "        self.function = None\n",
    "        \n",
    "    def load_function(self):\n",
    "        import pyximport\n",
    "        pyximport.install(reload_support=True)\n",
    "        self.function = getattr(__import__(self.module), self.name)\n",
    "        \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        if self.function is None:\n",
    "            self.load_function()\n",
    "\n",
    "        return self.function(*args, **kwargs)\n",
    "    \n",
    "    def __getstate__(self):\n",
    "        # Don't return the module so that each node has to recompile it itself.\n",
    "        state = self.__dict__.copy()\n",
    "        state['function'] = None\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_observations = cython_function('dipper', 'group_observations')\n",
    "detect_dippers = cython_function('dipper', 'detect_dippers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to recompile the cython code whenever needed.\n",
    "from importlib import reload\n",
    "import sys\n",
    "import pyximport\n",
    "pyximport.install(reload_support=True)\n",
    "\n",
    "try:\n",
    "    del sys.modules['dipper']\n",
    "except KeyError:\n",
    "    pass\n",
    "import dipper\n",
    "\n",
    "detect_dippers.function = None\n",
    "group_observations.function = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_dippers_row(row, verbose=False, return_mjd=False):\n",
    "    return detect_dippers(row['mjd'], row['filterid'], row['psfmag'],\n",
    "                          row['psfmagerr'], row['xpos'], row['ypos'],\n",
    "                          row['catflags'],\n",
    "                          verbose=verbose, return_mjd=return_mjd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a UDF for spark\n",
    "detect_dippers_udf = sparkfunc.udf(detect_dippers, returnType=pyspark_types.FloatType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the spark query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run and save the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Run on spark\n",
    "res = (\n",
    "    ztf.region(ra1=270, ra2=310, dec1=-10, dec2=40)\n",
    "    #ztf.region(ra1=295, ra2=296, dec1=20, dec2=21)\n",
    "    .exclude_duplicates()\n",
    "    .where(sparkfunc.col(\"nobs_avail\") > 20)\n",
    "    .select(\n",
    "        '*',\n",
    "        detect_dippers_udf(ztf['mjd'], ztf['filterid'], ztf['psfmag'], ztf['psfmagerr'], ztf['xpos'], ztf['ypos'], ztf['catflags']).alias('score')\n",
    "    )\n",
    "    .where(sparkfunc.col(\"score\") > 2.)\n",
    "    #.collect()\n",
    "    .write.parquet('./query_high_cadence_2.parquet')\n",
    ")\n",
    "\n",
    "#print(len(res))\n",
    "\n",
    "#import pickle\n",
    "#pickle.dump(res, open('test_query.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the query results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = spark_session.read.parquet('/epyc/data/boyajian/saved_queries/query_high_cadence_2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df = query.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42119it [01:01, 684.33it/s]\n"
     ]
    }
   ],
   "source": [
    "new_scores = []\n",
    "for idx, row in tqdm.tqdm(query_df.iterrows()):\n",
    "    new_scores.append(detect_dippers_row(row))\n",
    "\n",
    "query_df['new_score'] = new_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_df = query_df.sort_values('new_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_links(row):\n",
    "    print(\"http://simbad.u-strasbg.fr/simbad/sim-coo?Coord=%.6f%+.6f&CooFrame=FK5&CooEpoch=2000&CooEqui=2000&CooDefinedFrames=none&Radius=20&Radius.unit=arcsec&submit=submit+query&CoordList=\" % (row['ra'], row['dec']))\n",
    "    print(\"RA+Dec: %.6f%+.6f\" % (row['ra'], row['dec']))\n",
    "    print(\"RA:     %.6f\" % row['ra'])\n",
    "    print(\"Dec:    %.6f\" % row['dec'])\n",
    "\n",
    "def show_lightcurve(idx):\n",
    "    row = sort_df.iloc[idx]  \n",
    "    print_links(row)    \n",
    "    plot_lightcurve(row)\n",
    "\n",
    "    print(\"Score:  %.3f\" % detect_dippers_row(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3640713b50a74d83828812f2d980b817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='idx'), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.show_lightcurve>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import interact, IntSlider\n",
    "interact(show_lightcurve, idx=IntSlider(min=0, max=100, value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-AXS Spark",
   "language": "python",
   "name": "spark-smj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
